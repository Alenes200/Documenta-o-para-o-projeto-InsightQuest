# Documenta√ß√£o Completa: Pipeline de An√°lise e Modelagem Preditiva

## üìã √çndice
- [Objetivo do Projeto](#objetivo-do-projeto)
- [Fase 1: An√°lise Explorat√≥ria e Pr√©-processamento Inicial](#fase-1-an√°lise-explorat√≥ria-e-pr√©-processamento-inicial)
- [Fase 2: Pr√©-Processamento e Engenharia de Features](#fase-2-pr√©-processamento-e-engenharia-de-features)
- [Fase 3: Engenharia e Sele√ß√£o de Features](#fase-3-engenharia-e-sele√ß√£o-de-features)
- [Fase 4: Treinamento e Otimiza√ß√£o de Modelos](#fase-4-treinamento-e-otimiza√ß√£o-de-modelos)
- [Fase 5: Implanta√ß√£o e Infer√™ncia](#fase-5-implanta√ß√£o-e-infer√™ncia)
- [Refer√™ncias Bibliogr√°ficas](#refer√™ncias-bibliogr√°ficas)

## üéØ Objetivo do Projeto

Este documento detalha um pipeline de Machine Learning de ponta a ponta, projetado para processar, analisar e construir modelos preditivos a partir de um conjunto de dados. O objetivo final √© prever as vari√°veis **Target1**, **Target2**, e **Target3** com a maior acur√°cia poss√≠vel, comparando a performance de diferentes algoritmos de regress√£o.

---

## üß© Fase 1: An√°lise Explorat√≥ria e Pr√©-processamento Inicial

### Prop√≥sito Geral

Esta fase inicial tem como objetivo transformar o conjunto de dados brutos em uma base de dados estruturada, limpa e bem compreendida. As atividades aqui realizadas s√£o cruciais para diagnosticar a qualidade dos dados, identificar desafios e informar as estrat√©gias de limpeza, engenharia de features e modelagem que ser√£o aplicadas posteriormente.

Esta fase compreende duas atividades principais:
- **An√°lise Explorat√≥ria de Dados (EDA - Exploratory Data Analysis)**: Um processo investigativo para resumir as principais caracter√≠sticas dos dados, frequentemente com m√©todos visuais.
- **Limpeza de Dados (Data Cleaning)**: O processo de detectar e corrigir (ou remover) registros e colunas corrompidos, imprecisos ou irrelevantes do conjunto de dados.

### üì¶ BLOCO 1: Importa√ß√£o de Bibliotecas

**Objetivo**: Carregar as ferramentas (bibliotecas) necess√°rias para manipula√ß√£o, an√°lise e visualiza√ß√£o de dados.

**Descri√ß√£o Detalhada**:
- `pandas`: A principal biblioteca para manipula√ß√£o de dados em Python. √â utilizada para criar e gerenciar DataFrames, que s√£o estruturas de dados tabulares (semelhantes a planilhas) onde os dados do projeto residem.
- `numpy`: Fornece suporte para opera√ß√µes matem√°ticas e arrays multidimensionais, sendo a base para muitas funcionalidades do pandas.
- `matplotlib` e `seaborn`: Bibliotecas de visualiza√ß√£o de dados usadas para criar gr√°ficos e plots que ajudam a entender os padr√µes e as distribui√ß√µes nos dados.
- `warnings.filterwarnings('ignore')`: Suprime mensagens de aviso que podem poluir a sa√≠da do notebook, √∫til para manter o foco nos resultados.
- `pd.set_option(...)`: Configura√ß√µes do pandas para melhorar a exibi√ß√£o de DataFrames no ambiente de desenvolvimento, garantindo que mais colunas e linhas sejam vis√≠veis.

**Justificativa T√©cnica**: A importa√ß√£o de bibliotecas √© o ponto de partida padr√£o para qualquer script de an√°lise de dados. A utiliza√ß√£o deste conjunto de ferramentas, conhecido como a "Pilha Cient√≠fica do Python" (Scientific Python Stack), √© o padr√£o da ind√∫stria para projetos de Data Science.

### üìÇ BLOCO 2: Carregamento de Dados

**Objetivo**: Ingerir o conjunto de dados brutos do arquivo fonte para a mem√≥ria, dentro de um DataFrame do pandas.

**Descri√ß√£o Detalhada**: A fun√ß√£o `pd.read_excel('JogadoresV1.xlsx')` √© chamada para ler o arquivo Excel especificado. O conte√∫do √© carregado em uma vari√°vel chamada `df`. Imediatamente ap√≥s o carregamento, o atributo `.shape` do DataFrame √© verificado para confirmar as dimens√µes (n√∫mero de linhas e colunas), garantindo que a importa√ß√£o ocorreu como esperado.

**Justificativa T√©cnica**: Este √© o primeiro passo pr√°tico do pipeline. A verifica√ß√£o das dimens√µes √© uma valida√ß√£o inicial crucial para detectar problemas de leitura ou corrup√ß√£o de arquivos.

### üîç BLOCO 3: Explora√ß√£o Inicial (EDA)

**Objetivo**: Obter uma primeira vis√£o geral da estrutura e do conte√∫do do conjunto de dados.

**Descri√ß√£o Detalhada**:
- `df.head()`: Exibe as primeiras 5 linhas do DataFrame. √â usado para uma inspe√ß√£o visual r√°pida do formato dos dados e dos nomes das colunas.
- `df.info()`: Fornece um resumo t√©cnico conciso. Mostra o nome de cada coluna, o n√∫mero de valores n√£o nulos e o tipo de dado (dtype) inferido para cada uma.
- `df.describe()`: Gera estat√≠sticas descritivas para todas as colunas num√©ricas, incluindo contagem, m√©dia, desvio padr√£o, valor m√≠nimo, m√°ximo e os quartis.

**Justificativa T√©cnica**: Este bloco √© a ess√™ncia da An√°lise Explorat√≥ria de Dados. `info()` √© vital para identificar colunas com tipos de dados incorretos ou com valores ausentes. `describe()` √© fundamental para detectar poss√≠veis outliers (ex: valores m√≠nimos ou m√°ximos absurdos) e para entender a escala e a dispers√£o de cada vari√°vel.

### üßπ BLOCO 3.5: Tratamento Inicial de Tipos e Colunas

**Objetivo**: Realizar a primeira rodada de limpeza, focada em remover dados irrelevantes e corrigir os tipos de dados.

**Descri√ß√£o Detalhada**:
- **Remo√ß√£o de Coluna**: A coluna 'C√≥digo de Acesso' √© removida usando `df.drop()`.
- **Convers√£o de Tipo**: Uma lista de colunas que deveriam ser num√©ricas, mas podem conter texto, √© processada. A fun√ß√£o `pd.to_numeric` com o argumento `errors='coerce'` √© aplicada. Este argumento √© crucial: ele tenta converter cada valor para n√∫mero; se falhar (ex: encontrar um texto), ele substitui o valor problem√°tico por NaN (Not a Number), marcando-o para tratamento posterior.

**Justificativa T√©cnica**:
- Colunas de identifica√ß√£o, como c√≥digos de acesso, n√£o possuem valor preditivo e devem ser removidas para evitar que o modelo as interprete erroneamente como features.
- Garantir os tipos de dados corretos √© fundamental. Algoritmos de Machine Learning exigem entradas num√©ricas. A estrat√©gia `errors='coerce'` √© uma abordagem segura e robusta para lidar com dados "sujos" sem interromper o fluxo de execu√ß√£o.

### üî§ BLOCO 3.6: Identifica√ß√£o de Colunas N√£o-Num√©ricas

**Objetivo**: Programaticamente listar todas as colunas que n√£o s√£o num√©ricas ap√≥s a limpeza inicial.

**Descri√ß√£o Detalhada**: O m√©todo `df.select_dtypes(exclude=np.number)` filtra o DataFrame, selecionando apenas as colunas cujo tipo de dado n√£o √© num√©rico (como object, que geralmente armazena texto).

**Justificativa T√©cnica**: Este passo serve como uma verifica√ß√£o de sanidade para as etapas anteriores e cria uma lista de colunas que precisar√£o de tratamento especial mais adiante, como a aplica√ß√£o de t√©cnicas de encoding (BLOCO 16).

### üéØ BLOCO 4: An√°lise das Vari√°veis-Alvo (Targets)

**Objetivo**: Investigar em profundidade as vari√°veis que o modelo de Machine Learning tentar√° prever.

**Descri√ß√£o Detalhada**: Para cada uma das tr√™s colunas target, o c√≥digo calcula estat√≠sticas descritivas detalhadas (contagem de ausentes, m√≠nimo, m√°ximo, m√©dia, mediana, desvio padr√£o). Em seguida, um histograma √© plotado para cada target, mostrando visualmente a distribui√ß√£o de seus valores.

**Justificativa T√©cnica**: A an√°lise do target √© a etapa mais importante da EDA. A distribui√ß√£o dos targets informa sobre:
- **Skewness (Assimetria)**: Se a distribui√ß√£o for muito assim√©trica, pode ser necess√°rio aplicar transforma√ß√µes (como logar√≠tmica) para ajudar o modelo.
- **Outliers**: A presen√ßa de valores extremos pode impactar negativamente o treinamento de alguns modelos.
- **Escala**: Entender a faixa de valores (m√≠nimo e m√°ximo) √© crucial para interpretar as m√©tricas de erro do modelo (como o RMSE).

### üìä BLOCO 6 e 7: Diagn√≥stico de Qualidade de Dados

**Objetivo**: Quantificar sistematicamente dois dos problemas de qualidade de dados mais comuns: valores ausentes e valores negativos inv√°lidos.

**Descri√ß√£o Detalhada**:
- **Valores Ausentes**: O c√≥digo calcula a contagem e o percentual de valores NaN para cada coluna e exibe as 20 colunas mais afetadas em uma tabela e em um gr√°fico de barras.
- **Valores Negativos**: O c√≥digo itera sobre as colunas num√©ricas e conta quantos valores s√£o menores que zero, novamente apresentando um resumo tabular e gr√°fico.

**Justificativa T√©cnica**: Esta an√°lise √© um diagn√≥stico aprofundado. Os resultados guiar√£o a estrat√©gia de imputa√ß√£o (BLOCO 10.2), que √© a t√©cnica de preenchimento de valores ausentes. Identificar colunas com um percentual muito alto de aus√™ncia pode levar √† decis√£o de remov√™-las completamente. A an√°lise de negativos √© crucial para colunas onde eles s√£o semanticamente inv√°lidos (ex: tempo, quantidade).

### üìà BLOCO 8: An√°lise de Correla√ß√£o

**Objetivo**: Identificar quais features possuem a mais forte rela√ß√£o linear com as vari√°veis-alvo.

**Descri√ß√£o Detalhada**: Para cada target, o c√≥digo calcula o Coeficiente de Correla√ß√£o de Pearson entre ele e todas as outras features num√©ricas. Os resultados s√£o ordenados pelo valor absoluto da correla√ß√£o, e as 10 features mais correlacionadas s√£o visualizadas em um gr√°fico de barras.

**Justificativa T√©cnica**: A correla√ß√£o √© uma das t√©cnicas mais simples e eficazes para a sele√ß√£o inicial de features (Feature Selection). Features com alta correla√ß√£o (pr√≥xima de 1 ou -1) com o target s√£o fortes candidatas a serem preditores importantes. Esta an√°lise ajuda a focar os esfor√ßos de engenharia de features nas vari√°veis que j√° demonstram ter algum poder preditivo.

### üìã BLOCO 9: Resumo da An√°lise e Conclus√£o da Fase

**Objetivo**: Consolidar as descobertas da fase de an√°lise e formalizar a transi√ß√£o para a fase de tratamento e limpeza profunda.

**Descri√ß√£o Detalhada**: O c√≥digo cria um dicion√°rio de resumo com as principais m√©tricas do estado atual do dataset (n√∫mero de colunas com problemas, etc.) e define as pr√≥ximas etapas do pipeline.

**Justificativa T√©cnica**: Este bloco funciona como um "checkpoint" de qualidade e planejamento. Ele documenta o que foi aprendido durante a EDA e estabelece um plano de a√ß√£o claro para as pr√≥ximas fases, garantindo um fluxo de trabalho organizado e metodol√≥gico.

---

## üõ†Ô∏è Fase 2: Pr√©-Processamento e Engenharia de Features

### Prop√≥sito Geral

Com base no diagn√≥stico da Fase 1, esta fase executa a limpeza profunda e, mais importante, a cria√ß√£o de novas vari√°veis (features) para enriquecer o dataset. O objetivo √© transformar os dados em um formato otimizado para o treinamento de modelos de Machine Learning.

### üß© BLOCO 10, 10.1 e 10.2: Imputa√ß√£o Robusta e Preserva√ß√£o de Informa√ß√£o

**Objetivo**: Tratar sistematicamente todos os valores ausentes, negativos e inv√°lidos, utilizando estrat√©gias que minimizem a perda de informa√ß√£o.

**Descri√ß√£o Detalhada**:
- **Dados Categ√≥ricos (Bloco 10)**: Valores ausentes ou inv√°lidos em colunas categ√≥ricas s√£o substitu√≠dos pela moda (o valor mais frequente da coluna).
- **Dados Num√©ricos Negativos (Bloco 10.1)**: Valores negativos s√£o substitu√≠dos pela mediana dos valores positivos. Crucialmente, uma nova coluna "flag" (`_nao_respondeu`) √© criada para preservar a informa√ß√£o de que aquele dado era originalmente negativo.
- **Dados Num√©ricos Ausentes (Bloco 10.2)**: Colunas 100% vazias s√£o removidas. Nos demais casos, valores NaN s√£o preenchidos com a mediana da coluna, e uma "flag" (`_tinha_missing`) √© criada.

**Justificativa T√©cnica**: O uso da mediana para imputa√ß√£o num√©rica √© uma escolha robusta, pois n√£o √© afetada por outliers, ao contr√°rio da m√©dia. A cria√ß√£o de colunas "flag" √© uma t√©cnica avan√ßada que permite ao modelo aprender se a pr√≥pria aus√™ncia ou nega√ß√£o de uma resposta √© um padr√£o de comportamento preditivo, evitando a perda total dessa informa√ß√£o.

### üé® BLOCO 12 e 13: Processamento de Tipos de Dados Complexos

**Objetivo**: Converter tipos de dados n√£o-num√©ricos, como cores e datas, em m√∫ltiplas features num√©ricas que um modelo possa interpretar.

**Descri√ß√£o Detalhada**:
- **Cores (Bloco 12)**: C√≥digos hexadecimais s√£o convertidos para o sistema RGB. A partir da√≠, novas features como brilho, satura√ß√£o e flags bin√°rias (`_eh_cinza`) s√£o calculadas.
- **Datas (Bloco 13)**: A coluna de data/hora √© usada para extrair componentes como `dia_semana`, `hora_dia`, `mes`, `periodo_dia` e `eh_fim_semana`.

**Justificativa T√©cnica**: Modelos de Machine Learning n√£o conseguem interpretar um c√≥digo de cor ou uma data diretamente. Este processo "descompacta" a informa√ß√£o contida nesses formatos, transformando-a em vari√°veis num√©ricas que podem revelar padr√µes, como a influ√™ncia do per√≠odo do dia no desempenho do jogador.

### ‚úÖ BLOCO 14 e 15: Verifica√ß√£o de Qualidade e Corre√ß√µes Finais

**Objetivo**: Realizar um controle de qualidade final ap√≥s as transforma√ß√µes e remover features que n√£o agregam valor preditivo.

**Descri√ß√£o Detalhada**: O c√≥digo verifica sistematicamente se ainda restam valores ausentes, negativos ou infinitos. Em seguida, identifica e remove colunas com vari√¢ncia zero.

**Justificativa T√©cnica**: Uma coluna com vari√¢ncia zero cont√©m o mesmo valor para todos os jogadores. Ela √©, por defini√ß√£o, in√∫til para um modelo preditivo, pois n√£o ajuda a diferenciar um resultado do outro. Remov√™-las √© uma otimiza√ß√£o que simplifica o modelo sem perda de informa√ß√£o.

### üî¢ BLOCO 16: Encoding de Features Categ√≥ricas

**Objetivo**: Converter todas as colunas de texto restantes para um formato num√©rico.

**Descri√ß√£o Detalhada**: A t√©cnica de Frequency Encoding √© aplicada. Cada categoria de texto (ex: "Solteiro") √© substitu√≠da por sua frequ√™ncia de apari√ß√£o no dataset (ex: 0.45, se 45% dos jogadores forem solteiros).

**Justificativa T√©cnica**: Esta √© a etapa final para garantir que 100% das features de entrada do modelo sejam num√©ricas. O Frequency Encoding √© uma t√©cnica eficiente que captura a "popularidade" de uma categoria em uma √∫nica vari√°vel num√©rica, evitando a cria√ß√£o de m√∫ltiplas colunas como faria o One-Hot Encoding.

### üß† BLOCO 17, 18 e 19: Engenharia de Features Avan√ßada

**Objetivo**: Criar novas features de alto valor a partir de combina√ß√µes das vari√°veis existentes, baseando-se no conhecimento do problema.

**Descri√ß√£o Detalhada**:
- **Desempenho (Bloco 17)**: Cria√ß√£o de m√©tricas como `taxa_acerto`, `tempo_por_questao` e `evolucao_desempenho` (comparando rodadas).
- **Contexto (Bloco 18)**: Cria√ß√£o de scores para `qualidade_sono`, `nivel_social` (est√≠mulo do ambiente) e `media_emocional` (a partir de escalas Likert).
- **Intera√ß√£o (Bloco 19)**: Combina√ß√£o de features de diferentes dom√≠nios, como `desempenho_sono` (taxa de acerto * qualidade do sono).

**Justificativa T√©cnica**: Esta √© a etapa mais crucial para aumentar o poder preditivo de um modelo. Enquanto as features originais descrevem o que aconteceu, as features de engenharia descrevem como e por que aconteceu. Features de intera√ß√£o, em particular, permitem que o modelo capture rela√ß√µes n√£o-lineares complexas que n√£o seriam vis√≠veis de outra forma.

### üíæ BLOCO 20: Conclus√£o da Fase e Gera√ß√£o de Artefato de Dados

**Objetivo**: Finalizar o pr√©-processamento, realizar uma √∫ltima verifica√ß√£o e salvar o dataset final e enriquecido.

**Descri√ß√£o Detalhada**: O c√≥digo executa um resumo final do estado do dataset (dimens√µes, tipos de features criadas, verifica√ß√£o de qualidade). Em seguida, o DataFrame processado √© salvo em um arquivo `dados_processados.csv`.

**Justificativa T√©cnica**: Salvar o resultado em um arquivo CSV cria um artefato de dados. Isso √© fundamental para a reprodutibilidade e efici√™ncia. A fase de modelagem pode agora come√ßar diretamente a partir deste arquivo limpo, sem precisar re-executar todo o pipeline de limpeza e engenharia de features a cada vez, garantindo consist√™ncia entre os experimentos.

### üîç BLOCO 21: An√°lise de Clusteriza√ß√£o e Descoberta de Perfis

#### Prop√≥sito Geral

Esta fase representa uma transi√ß√£o crucial da limpeza de dados para a engenharia de features inteligente. Utilizando t√©cnicas de aprendizado n√£o supervisionado, o objetivo √© investigar se os jogadores podem ser agrupados em perfis ou "clusters" com base em suas caracter√≠sticas e comportamentos. A descoberta de tais grupos permite criar uma nova e poderosa feature categ√≥rica (o ID do Cluster) que informa aos modelos preditivos a que "tribo" de comportamento um jogador pertence. Em vez de modelar uma popula√ß√£o homog√™nea, passamos a modelar subgrupos distintos, o que pode aumentar drasticamente a acur√°cia.

#### 1. Prepara√ß√£o e Normaliza√ß√£o dos Dados

**Objetivo**: Preparar o dataset limpo para ser analisado por algoritmos de clusteriza√ß√£o baseados em dist√¢ncia.

**Descri√ß√£o Detalhada**: O pipeline inicia com o `dados_processados.csv`. As colunas Target s√£o removidas, pois a clusteriza√ß√£o deve descobrir padr√µes inerentes aos dados de entrada, sem o vi√©s da vari√°vel de sa√≠da. Em seguida, todas as features num√©ricas s√£o selecionadas e normalizadas utilizando o `RobustScaler`.

**Justificativa T√©cnica**: A normaliza√ß√£o √© um passo mandat√≥rio para a clusteriza√ß√£o. O RobustScaler foi escolhido especificamente por sua resist√™ncia a outliers. Ao centralizar os dados com a mediana e escalonar pelo intervalo interquartil, ele garante que features com valores extremos n√£o dominem o c√°lculo de dist√¢ncia, resultando em clusters mais est√°veis e significativos.

#### 2. Determina√ß√£o da Estrutura √ìtima (N√∫mero de Clusters e Algoritmo)

**Objetivo**: Encontrar o n√∫mero mais natural de grupos (K) nos dados e o algoritmo que melhor os separa.

**Descri√ß√£o Detalhada**: Uma abordagem multifacetada foi utilizada:
- **An√°lise de M√©tricas**: O pipeline testou um intervalo de K de 2 a 10. Para cada K, foram calculadas quatro m√©tricas de qualidade: In√©rcia (M√©todo do Cotovelo), Score de Silhueta, Davies-Bouldin e Calinski-Harabasz.
- **Vota√ß√£o**: Os resultados das m√©tricas foram un√¢nimes, com o Score de Silhueta (m√°ximo), Davies-Bouldin (m√≠nimo) e Calinski-Harabasz (m√°ximo) todos apontando para K=2 como a solu√ß√£o √≥tima.
- **Compara√ß√£o de Algoritmos**: Com K=2, tr√™s algoritmos foram comparados: K-Means, Hierarchical Clustering e Gaussian Mixture Models (GMM). O K-Means alcan√ßou o maior Score de Silhueta (0.735), indicando a melhor qualidade de clusteriza√ß√£o (clusters densos e bem separados).

**Justificativa T√©cnica**: Confiar em um √∫nico m√©todo para determinar K √© arriscado. A utiliza√ß√£o de um sistema de "vota√ß√£o" com m√∫ltiplas m√©tricas que avaliam diferentes aspectos da qualidade do cluster (coes√£o, separa√ß√£o) torna a escolha de K=2 extremamente robusta. O valor de silhueta de 0.735 √© considerado excelente, sugerindo que a divis√£o em dois grupos √© uma estrutura muito forte e natural nos dados. A compara√ß√£o de algoritmos valida que o K-Means, com suas premissas de clusters esf√©ricos, √© o mais adequado para a geometria destes dados.

#### 3. Caracteriza√ß√£o e Interpreta√ß√£o dos Perfis

**Objetivo**: Traduzir os clusters matem√°ticos em perfis de jogadores compreens√≠veis e acion√°veis.

**Descri√ß√£o Detalhada**:
- **Distribui√ß√£o**: Foram identificados dois grupos: Cluster 0 (majorit√°rio), com 134 jogadores (77.5%), e Cluster 1 (minorit√°rio), com 39 jogadores (22.5%).
- **An√°lise de Diferen√ßas**: A an√°lise das m√©dias das features revelou um padr√£o claro e consistente. O Cluster 1 √© fortemente caracterizado por valores significativamente mais altos em features relacionadas √† percep√ß√£o e escolha de cores, como satura√ß√£o (`Cor0206_saturacao`), brilho (`F0207_brilho`) e o uso de cores prim√°rias ou preto/branco (`F0207_eh_branco`, `Cor0208_eh_preto`). O Cluster 0, em contrapartida, apresenta valores baixos ou nulos nessas mesmas dimens√µes.
- **Visualiza√ß√£o**: A redu√ß√£o de dimensionalidade com PCA confirmou visualmente a exist√™ncia de dois grupos distintos e bem separados no espa√ßo de features.

**Justificativa T√©cnica**: Um cluster s√≥ tem valor se for interpret√°vel. A an√°lise de caracteriza√ß√£o demonstrou que a principal linha divis√≥ria entre os jogadores n√£o √© o tempo de resposta ou a taxa de acerto, mas sim uma dimens√£o de comportamento mais sutil ligada √†s intera√ß√µes com cores. Esta descoberta √© uma forma de engenharia de features autom√°tica, onde o algoritmo revelou um padr√£o latente que seria dif√≠cil de hipotetizar manualmente.

#### 4. Valida√ß√£o de Estabilidade e Gera√ß√£o de Artefatos

**Objetivo**: Garantir que os clusters s√£o robustos e salvar os resultados de forma que possam ser usados nas pr√≥ximas fases do pipeline.

**Descri√ß√£o Detalhada**:
- **Teste de Estabilidade (Bootstrap)**: O processo de clusteriza√ß√£o foi repetido 30 vezes em amostras aleat√≥rias dos dados. A estabilidade m√©dia de 83.3% (a frequ√™ncia com que um ponto de dados permaneceu em seu cluster original) confirma que os perfis encontrados s√£o altamente est√°veis e n√£o um resultado do acaso.
- **Gera√ß√£o de Artefatos**: O pipeline salvou tr√™s sa√≠das essenciais:
  - `jogadores_clusterizados_v2.csv`: O dataset enriquecido com a nova coluna Cluster.
  - `clusters_metadados.csv`: Um resumo da composi√ß√£o dos clusters.
  - `clustering_artifacts_v2.pkl`: Um arquivo contendo o RobustScaler ajustado e o modelo KMeans treinado.

**Justificativa T√©cnica**: A valida√ß√£o de estabilidade confere um alto grau de confian√ßa cient√≠fica aos resultados. A gera√ß√£o do arquivo de artefatos `.pkl` √© o passo mais cr√≠tico para a operacionaliza√ß√£o. Ele permite que, na fase de infer√™ncia, um novo jogador passe pelo mesmo scaler e seja classificado pelo mesmo modelo K-Means, garantindo que o perfil atribu√≠do seja consistente com a an√°lise original. Isso transforma a clusteriza√ß√£o de um exerc√≠cio anal√≠tico em uma ferramenta preditiva reutiliz√°vel.

---

## ‚öôÔ∏è Fase 3: Engenharia e Sele√ß√£o de Features

### Prop√≥sito Geral

Esta fase transforma um dataset j√° limpo e clusterizado em m√∫ltiplos conjuntos de dados altamente otimizados, cada um especificamente ajustado para prever uma das vari√°veis-alvo. As atividades s√£o divididas em duas grandes etapas:
- **Engenharia de Features**: Cria√ß√£o de novas colunas a partir das existentes para capturar intera√ß√µes complexas e padr√µes n√£o-lineares.
- **Sele√ß√£o de Features**: Utiliza√ß√£o de um m√©todo h√≠brido e robusto para classificar a import√¢ncia de todas as features e selecionar apenas as "melhores" para cada alvo.

### üî¨ BLOCO 1.1: A Fun√ß√£o H√≠brida de Sele√ß√£o de Features (`select_best_features_hybrid`)

**Objetivo**: Avaliar a import√¢ncia de cada feature em rela√ß√£o a um target espec√≠fico, utilizando uma combina√ß√£o ponderada de m√∫ltiplas t√©cnicas, para produzir um ranking robusto e confi√°vel.

**Descri√ß√£o Detalhada**: Esta fun√ß√£o n√£o confia em um √∫nico m√©todo. Em vez disso, ela agrega a "opini√£o" de seis abordagens diferentes:
1. **Correla√ß√£o de Pearson (Linear)**: Mede a rela√ß√£o linear entre a feature e o alvo.
2. **Correla√ß√£o de Spearman (Monot√¥nica)**: Mede a rela√ß√£o monot√¥nica (se uma vari√°vel aumenta, a outra tamb√©m, mas n√£o necessariamente a uma taxa constante). √â menos sens√≠vel a outliers.
3. **Informa√ß√£o M√∫tua** (`mutual_info_regression`): Captura qualquer tipo de rela√ß√£o estat√≠stica, incluindo as n√£o-lineares.
4. **Teste F** (`f_regression`): Um teste estat√≠stico que avalia se a feature tem uma rela√ß√£o linear significativa com o alvo.
5. **Import√¢ncia de Features (XGBoost)**: Treina um modelo XGBoost e extrai a import√¢ncia que o modelo atribuiu a cada feature para fazer suas previs√µes.
6. **Import√¢ncia de Features (LightGBM & RandomForest)**: O mesmo processo, mas usando outros dois modelos baseados em √°rvores, cada um com suas pr√≥prias nuances.

As pontua√ß√µes de cada m√©todo s√£o normalizadas (colocadas em uma escala de 0 a 1) e combinadas em uma pontua√ß√£o final ponderada. As features com as maiores pontua√ß√µes combinadas s√£o selecionadas.

**C√≥digo Cr√≠tico**: A ess√™ncia da fun√ß√£o est√° na agrega√ß√£o ponderada das pontua√ß√µes normalizadas. Os pesos (ex: `0.30 * normalize(corr_series)`) foram definidos para dar mais import√¢ncia aos m√©todos de correla√ß√£o e √† robustez do XGBoost.

```python
# Normaliza uma s√©rie de pontua√ß√µes para uma escala de 0 a 1
normalize = lambda s: (s - s.min()) / (s.max() - s.min() + 1e-10)

# Combina as pontua√ß√µes normalizadas de 6 m√©todos com pesos diferentes
combined_score = (0.30 * normalize(corr_series) +      # Correla√ß√µes (Pearson+Spearman)
                  0.15 * normalize(mi_series) +        # Informa√ß√£o M√∫tua
                  0.15 * normalize(f_series) +         # Teste F
                  0.20 * normalize(xgb_series) +       # Import√¢ncia do XGBoost
                  0.10 * normalize(lgb_series) +       # Import√¢ncia do LightGBM
                  0.10 * normalize(rf_series))         # Import√¢ncia do RandomForest

# Seleciona as N features com a maior pontua√ß√£o combinada
selected = combined_score.nlargest(n_features).index.tolist()
return selected
```

**Justificativa T√©cnica**: Confiar em um √∫nico m√©todo de sele√ß√£o de features √© arriscado. Um m√©todo pode ser enviesado para um certo tipo de rela√ß√£o (ex: correla√ß√£o s√≥ pega rela√ß√µes lineares). Ao combinar m√∫ltiplas perspectivas, a sele√ß√£o se torna muito mais robusta. M√©todos baseados em modelos (como XGBoost) s√£o particularmente poderosos porque avaliam a utilidade de uma feature no contexto de outras, capturando intera√ß√µes.

**Refer√™ncia e Conceito**:
- **Ensemble Feature Selection**: O conceito de combinar m√∫ltiplos m√©todos de sele√ß√£o para obter um resultado mais est√°vel e confi√°vel. Este princ√≠pio √© an√°logo aos modelos de ensemble (como o Random Forest), que combinam m√∫ltiplos modelos fracos para criar um modelo forte. [Refer√™ncia: Saeys, Y., Inza, I., & Larra√±aga, P. (2007). A review of feature selection techniques in bioinformatics. Bioinformatics, 23(19), 2507-2517.]

### üèóÔ∏è BLOCO 1.2: O Pipeline Principal (`feature_engineering_pipeline`)

Este pipeline organiza todo o processo em quatro etapas sequenciais.

#### Etapa 1: Carregamento e Prepara√ß√£o Inicial

**Objetivo**: Carregar os dados clusterizados e prepar√°-los para o processamento, separando features (X) e alvos (y).

**Descri√ß√£o Detalhada**: Carrega o arquivo `jogadores_clusterizados_v2.csv` e remove as linhas onde qualquer um dos targets √© nulo, garantindo que o pipeline trabalhe apenas com dados completos e utiliz√°veis para treinamento supervisionado.

#### Etapa 2: Limpeza Final e Estabiliza√ß√£o Num√©rica

**Objetivo**: Garantir que o dataset esteja numericamente est√°vel e livre de redund√¢ncia.

**Descri√ß√£o Detalhada**:
- **Imputa√ß√£o Final**: Preenche quaisquer valores nulos restantes com a mediana da coluna, e depois com 0, para garantir que n√£o haja NaNs.
- **Tratamento de Infinitos**: Substitui valores `np.inf` (que podem surgir de divis√µes por zero) por 0.
- **Remo√ß√£o de Vari√¢ncia Zero**: Remove colunas que n√£o t√™m varia√ß√£o (todos os valores s√£o iguais), pois n√£o carregam informa√ß√£o preditiva.
- **Remo√ß√£o de Alta Colinearidade**: Calcula a matriz de correla√ß√£o entre todas as features. Se duas features tiverem uma correla√ß√£o maior que 0.97, uma delas √© removida.

**Justificativa T√©cnica**:
- **Colinearidade**: Manter duas features que s√£o quase id√™nticas (alta correla√ß√£o) √© redundante. Isso pode desestabilizar alguns modelos (especialmente regress√µes lineares) e aumentar a complexidade do modelo sem adicionar nova informa√ß√£o. Remover uma delas simplifica o problema. A escolha de 0.97 √© um limiar estrito, mas comum, para remover apenas a redund√¢ncia mais √≥bvia.

**Refer√™ncia e Conceito**:
- **Multicollinearity**: Um fen√¥meno no qual uma vari√°vel preditora em um modelo de regress√£o pode ser prevista linearmente a partir das outras com um grau substancial de precis√£o. [Refer√™ncia: "A Beginner's Guide to Multicollinearity and VIF," da Statology.]

#### Etapa 3: Engenharia de Features Avan√ßada

**Objetivo**: Criar novas features inteligentes para aumentar o poder preditivo do dataset.

**Descri√ß√£o Detalhada**:
- **One-Hot Encoding do Cluster**: A coluna categ√≥rica Cluster √© transformada em m√∫ltiplas colunas bin√°rias (ex: `Cluster_0`, `Cluster_1`).
- **Intera√ß√µes com o Cluster**: Para cada feature num√©rica, uma nova feature √© criada calculando a diferen√ßa entre o valor do jogador e a m√©dia do cluster ao qual ele pertence (ex: `T01_vs_cluster_mean`).
- **Agrega√ß√µes por Prefixo**: Para grupos de colunas com o mesmo prefixo (ex: `Q0...`, `P...`), s√£o calculadas estat√≠sticas agregadas como m√©dia, desvio padr√£o e range.
- **Transforma√ß√µes N√£o-Lineares**: Para as 10 features mais correlacionadas com cada target, s√£o criadas novas features aplicando transforma√ß√µes de quadrado (`**2`) e raiz quadrada (`sqrt`).

**Justificativa T√©cnica**:
- **One-Hot Encoding**: √â a maneira padr√£o de representar uma vari√°vel categ√≥rica para modelos de machine learning, permitindo que o modelo aprenda pesos espec√≠ficos para cada categoria de cluster.
- **Intera√ß√µes com Cluster**: Esta √© uma t√©cnica poderosa. A feature `T01_vs_cluster_mean` n√£o mede apenas o tempo T01, mas mede o qu√£o anormal √© o tempo daquele jogador em rela√ß√£o ao seu pr√≥prio grupo. Isso normaliza o comportamento e pode ser um preditor muito mais forte.
- **Transforma√ß√µes N√£o-Lineares**: Muitos modelos (especialmente os lineares) t√™m dificuldade em capturar rela√ß√µes n√£o-lineares. Adicionar o quadrado de uma feature (`X**2`) permite que um modelo linear se ajuste a uma par√°bola, aumentando sua flexibilidade e poder de captura de padr√µes complexos.

#### Etapa 4: Sele√ß√£o H√≠brida de Features por Target

**Objetivo**: Aplicar a fun√ß√£o de sele√ß√£o h√≠brida para cada target, selecionando um n√∫mero diferente de features conforme a estrat√©gia definida.

**Descri√ß√£o Detalhada**: O c√≥digo itera sobre os tr√™s targets. Para Target1 e Target3, ele chama a fun√ß√£o `select_best_features_hybrid` pedindo as 120 melhores features. Para Target2, ele adota uma estrat√©gia diferente, pedindo as 180 melhores.

**Justificativa T√©cnica**: Esta √© a implementa√ß√£o da estrat√©gia adaptativa. A hip√≥tese √© que Target2 pode ser um fen√¥meno mais complexo ou sutil, beneficiando-se de um conjunto maior e mais diversificado de preditores. Em vez de usar um √∫nico conjunto de features para todos os modelos, o pipeline cria um "time de especialistas" de features para cada tarefa de previs√£o.

### üöÄ Execu√ß√£o e Sa√≠das (`if __name__ == '__main__':`)

**Objetivo**: Orquestrar a execu√ß√£o do pipeline e salvar os resultados em arquivos para uso na pr√≥xima fase (modelagem).

**Descri√ß√£o Detalhada**:
- Executa a fun√ß√£o `feature_engineering_pipeline`.
- Salva o DataFrame completo com todas as features de engenharia em `X_engineered.csv`.
- Salva os targets em `y_targets.csv`.
- Salva as listas de features selecionadas para cada target em `selected_features.json`.
- Salva um relat√≥rio de todas

Refer√™ncias Bibliogr√°ficas
Saeys, Y., Inza, I., Larraaga, P. (2007). A review of feature selection techniques in bioinformatics. Bioinformatics, 23(19), 2507-2517.
Bergstra, J., Bengio, Y. (2012). Random Search for Hyper-Parameter Optimization. Journal of Machine Learning Research, 13, 281-305.
Wolpert, D. H. (1992). Stacked generalization. Neural networks, 5(2), 241-259.
Documenta√ß√£o de Scikit-learn sobre Model Persistence.
Artigos e conceitos gerais sobre multicolinearidade, valida√ß√£o cruzada e otimiza√ß√£o de hiperpar√¢metros (mencionados no conte√∫do).
